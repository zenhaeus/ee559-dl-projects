{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â FancyNet2\n",
    "FancyNet2 applies a convolutional network to both channels of the input data separately. I added one more convolution with respect to FancyNet1. The output feature vectors of the conv. net are then used in two different ways:\n",
    "- Directly fed into digit classifier -> Classify digits\n",
    "- Concatenated to form a feature vector of 2 x output of convnet. Then fed into classifier for final binary decision\n",
    "\n",
    "The training is based on the weighted sum of three losses:\n",
    "- Loss from digit classifier of channel 0\n",
    "- Loss from digit classifier of channel 1\n",
    "- Loss from final binary classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import dlc_practical_prologue as prologue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data):\n",
    "    mu, std = data[0].mean(), data[0].std()\n",
    "    \n",
    "    data[0].sub_(mu).div_(std)\n",
    "    data[3].sub_(mu).div_(std)\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = prologue.generate_pair_sets(1000)\n",
    "data = normalize_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FancyNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FancyNet2, self).__init__()\n",
    "        \n",
    "        # convolutional network for feature extraction\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, kernel_size=3),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 512, kernel_size=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # fully connected layer for digit classification\n",
    "        self.classifierNumber = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "        \n",
    "        # fully connected layer for final decision\n",
    "        self.classifierFinal = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(2*512, 1000),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(1000, 2),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # classification of digit of channel 0\n",
    "        x_num1 = self.features(x[:,0:1,:,:])\n",
    "        x_num1 = x_num1.view(-1, 512)\n",
    "        \n",
    "        # classification of digit of channel 1\n",
    "        x_num2 = self.features(x[:,1:,:,:])\n",
    "        x_num2 = x_num2.view(-1, 512)\n",
    "        \n",
    "        x_f = torch.cat((x_num1, x_num2), 1)\n",
    "\n",
    "        x_num1 = self.classifierNumber(x_num1)\n",
    "        x_num2 = self.classifierNumber(x_num2)\n",
    "        x_f = self.classifierFinal(x_f)\n",
    "        \n",
    "        return x_num1, x_num2, x_f "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, nb_epochs, lr):\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr, momentum = 0.3)\n",
    "    #optimizer = optim.Adam(model.parameters(), lr)\n",
    "    \n",
    "    # weights for different losses\n",
    "    w_1 = 1\n",
    "    w_2 = 1\n",
    "    w_f = 1\n",
    "\n",
    "    for e in range(nb_epochs):\n",
    "        sum_loss = 0\n",
    "        for b in range(0, train_data[0].size(0), mini_batch_size):\n",
    "            out_1, out_2, out_f = model(train_data[0].narrow(0, b, mini_batch_size))\n",
    "            \n",
    "            # losses of digit classification\n",
    "            loss_1 = criterion(out_1, train_data[2][b:b + mini_batch_size, 0])\n",
    "            loss_2 = criterion(out_2, train_data[2][b:b + mini_batch_size, 1])\n",
    "            \n",
    "            # loss of final classification\n",
    "            loss_f = criterion(out_f, train_data[1].narrow(0, b, mini_batch_size))\n",
    "            \n",
    "            # total loss\n",
    "            loss = w_1 * loss_1 + w_2 * loss_2 + w_f * loss_f\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss = sum_loss + loss.item()\n",
    "            \n",
    "        print(e, sum_loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors(model, data):\n",
    "\n",
    "    nb_num1_errors = 0\n",
    "    nb_num2_errors = 0\n",
    "    nb_final_errors = 0\n",
    "\n",
    "    for b in range(0, data[0].size(0), mini_batch_size):\n",
    "        out_1, out_2, out_f = model(data[0].narrow(0, b, mini_batch_size))\n",
    "        \n",
    "        pred_num_1 = torch.argmax(out_1, 1)\n",
    "        pred_num_2 = torch.argmax(out_2, 1)\n",
    "        predicted_decision = torch.argmax(out_f, 1)\n",
    "\n",
    "        for k in range(mini_batch_size):\n",
    "            if data[2][b + k, 0] != pred_num_1[k] :\n",
    "                nb_num1_errors = nb_num1_errors + 1\n",
    "            if data[2][b + k, 1] != pred_num_2[k] :\n",
    "                nb_num2_errors = nb_num2_errors + 1\n",
    "            if data[1][b + k] != predicted_decision[k] :\n",
    "                nb_final_errors = nb_final_errors + 1\n",
    "\n",
    "    return nb_num1_errors, nb_num2_errors, nb_final_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mini_batch_size = 100\n",
    "nb_epochs = 50\n",
    "lr = 1e-1\n",
    "\n",
    "fancyNet2 = FancyNet2()\n",
    "train_model(fancyNet2, data, nb_epochs, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_train_errors = compute_nb_errors(fancyNet2, data[0:3])\n",
    "nb_test_errors = compute_nb_errors(fancyNet2, data[3:6])\n",
    "\n",
    "train_size = data[0].size(0)\n",
    "test_size = data[3].size(0)\n",
    "\n",
    "\n",
    "print('train errors: \\n \\t num 1 errors : {:0.2f}% {:g}/{:g} \\n \\\n",
    "\\t num 2 errors : {:0.2f}% {:g}/{:g} \\n \\\n",
    "\\t final errors : {:0.2f}% {:g}/{:g}'.format((100 * nb_train_errors[0]) / train_size, nb_train_errors[0], train_size,\n",
    "(100 * nb_train_errors[1]) / train_size, nb_train_errors[1], train_size,\n",
    "(100 * nb_train_errors[2]) / train_size, nb_train_errors[2], train_size))\n",
    "\n",
    "print('test errors: \\n \\t num 1 errors : {:0.2f}% {:g}/{:g} \\n \\\n",
    "\\t num 2 errors : {:0.2f}% {:g}/{:g} \\n \\\n",
    "\\t final errors : {:0.2f}% {:g}/{:g}'.format((100 * nb_test_errors[0]) / test_size, nb_test_errors[0], test_size,\n",
    "(100 * nb_test_errors[1]) / test_size, nb_test_errors[1], test_size,\n",
    "(100 * nb_test_errors[2]) / test_size, nb_test_errors[2], test_size))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dl]",
   "language": "python",
   "name": "conda-env-dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
