\documentclass[10pt,conference,compsocconf]{IEEEtran}

\usepackage{hyperref}
\usepackage{graphicx}	% For figure environment
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{tikz}
\usepackage{tikz-qtree}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{cleveref}
\usepackage{xcolor}
\usepackage{nicefrac}
\usetikzlibrary{trees,calc,arrows.meta,positioning,decorations.pathreplacing,bending}


\begin{document}
\title{Deep Learning - Miniproject 2}

\author{
  Olesya Altunina (285467), Mauro Pfister (235440), Joey Zenhaeusern (226652)
}

\maketitle

\begin{abstract}
This report is a description of our approach to implement a simple deep-learning framework from scratch. The framework is then used to build a very basic network and train, as well as evaluate it on a randomly generated dataset.
\end{abstract}

\section{Introduction}

\section{Framework implementation}
In order to implement our own mini deep-learning framework MyTorch we chose a structure based on the suggestion provided in the project description. All modules are implemented as classes which inherit from a parent class \texttt{Module}. They need to override the \texttt{forward()}, \texttt{backward()} and \texttt{param()} methods.

MyTorch supports batch processing which significantly decreases training times as well as prediction times on large networks. Currently, we do not support multiple forward passes since the inputs are not saved into a buffer. While it is still possible to run several forward passes one should keep in mind that the backward pass will only be with respect to the last applied input.

\subsection{Linear module}
% TODO: describe module
% TODO: mention weight initialization
% TODO: mention that parameter gradient is averaged over minibatches

\subsection{Sequential module}
% This is quite a lot of bullshitting ... What else could we write here?
The sequential module takes a chronological list of other modules as input and groups them in one module for convenience. The forward pass sequentially executes the forward passes of the individual modules. The backward pass does the same but in reversed order.

\subsection{ReLU module}
% TODO: describe module

\subsection{Tanh module}
% TODO: describe module

\subsection{MSE loss module}
The \texttt{LossMSE} class implements a mean squared error loss function defined as follows:
\begin{equation} \label{eq:MSE}
  l(x,t) = \frac{1}{n} \sum_1^n{(x_i - t_i)^2}, 
\end{equation}
where $n$ defines the dimension of $x$ and $t$. Note that we currently do not support the computation of the loss over a batch.

\subsection{Optimizer}
Additionally to the required modules above we also defined an optimizer class \texttt{Optimizer} that can be used to implement different optimizers. It takes the parameters of a model as input and contains a \texttt{step()} method to perform a gradient step on the model parameters. A second method \texttt{zero\_grad()} can be used to set the gradients of all optimized parameters to zero. 

Currently, only a stochastic gradient descent optimizer \texttt{SGD} is available,that can be used with or without momentum. The implementation of momentum is based on the one in PyTorch which differs from Sutskever et. al. \cite{sutskever2013importance}. The update of parameter $p$ is defined as follows 
\begin{equation} \label{eq:SGDmoment}
  \begin{aligned}
    v &= \rho v + g \\
    p &= p - \alpha v,
  \end{aligned}
\end{equation}
where $\alpha$, $\rho$ and $g$ denote learning rate, momentum and parameter gradient respectively.

\section{Framework validation}
To ensure the correctness of our implementations we wrote a series of unit tests which compare the functionalities of our code to the corresponding PyTorch functionalities. Currently the forward and backward passes of \texttt{Linear}, \texttt{ReLU} and \texttt{Tanh} modules can be tested \textcolor{red}{ADD MORE}.

Additionally, we created the same network architecture once in our framework and once in PyTorch and initialized all model parameters to a value of $10^{-6}$. This ensures that both models have exact the same initial condition. During training the total loss per epochs did not differ by more than $10^{-6}$ and the training and test accuracies matched perfectly. Therefore we conclude that within the limitations of our framework, the results should always be close to an equivalent PyTorch implementation.

\section{Data description}
According to the task description the training and test data sets consist each of 1000 points sampled uniformly in $[0,1]^2$. Class 0 is defined by all the points outside the disk with radius $\nicefrac{1}{\sqrt{2\pi}}$ and center $(\nicefrac{1}{2}, \nicefrac{1}{2})$ while class 1 denotes all the points inside that disk (see Figure~\ref{fig:data_set}).

\begin{figure}[h]
  \centering
  \includegraphics[width = 0.4\textwidth]{fig/disk_data_set.eps}
  \caption{Visualization of data set.}
  \label{fig:data_set}
\end{figure}


\section{Network description}
The network used for classification of the data set consists of two input units, three hidden layers of 25 units and two output units. We chose to use ReLU as activation functions for all layers.

\section{Performance evaluation}
% TODO: mention that for 25 epochs there is quite a bit of variation for both models -> might be due to weight initialization
% TODO: plot the prediction of the model on the data set -> maybe run 10 times and even show std ?


\section{Conclusion}

\bibliographystyle{IEEEtran}
\bibliography{literature}

% reference to Bishop for Newton's method

\end{document}
