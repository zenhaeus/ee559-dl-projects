\documentclass[10pt,conference,compsocconf]{IEEEtran}

\usepackage{hyperref}
\usepackage{graphicx}	% For figure environment
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{tikz}
\usepackage{tikz-qtree}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{cleveref}
\usepackage{xcolor}
\usepackage{nicefrac}
\usetikzlibrary{trees,calc,arrows.meta,positioning,decorations.pathreplacing,bending}


\begin{document}
\title{Deep Learning - Miniproject 2}

\author{
  Olesya Altunina (285467), Mauro Pfister (235440), Joey Zenhaeusern (226652)
}

\maketitle

\begin{abstract}
This report is a description of our approach to implement a simple deep-learning framework from scratch. The framework is then used to build a very basic network and train, as well as evaluate it on a randomly generated dataset.
\end{abstract}

\section{Introduction}

\section{Framework implementation}
In order to implement the mini deep-learning framework we chose a structure based on the suggestion provided in the project description. All modules are implemented as classes which inherit from a parent class \texttt{Module}. They need to override the \texttt{forward()}, \texttt{backward()} and \texttt{param()} methods.

To ensure the correctness of our implementations we wrote a series of unit tests which compare the functionalities of our code to the corresponding PyTorch functionalities.

% TODO: describe interesting implementation details
% TODO: clearly state how several consecutive forward passes are handled
% TODO: state limitations of implementation

\subsection{Linear module}
% TODO: describe module
% TODO: mention weight initialization

\subsection{Sequential module}
% This is quite a lot of bullshitting ... What else could we write here?
The sequential module takes a chronological list of other modules as input and groups them in one module for convenience. The forward pass sequentially executes the forward passes of the individual modules. The backward pass does the same but in reversed order.

\subsection{ReLU module}
% TODO: describe module

\subsection{Tanh module}
% TODO: describe module

\subsection{MSE loss module}
The \texttt{LossMSE} class implements a mean squared error loss function defined as follows:
\begin{equation} \label{eq:MSE}
  l(x,t) = \frac{1}{n} \sum_1^n{(x_i - t_i)^2}, 
\end{equation}
where $n$ defines the dimension of $x$ and $t$. Note that we currently do not support the computation of the loss over a batch.

\subsection{Optimizer}
Additionally to the required modules above we also defined an optimizer class \texttt{Optimizer} that can be used to implement different optimizers. It takes the parameters of a model as input and contains a \texttt{step()} method to perform a gradient step on the model parameters. A second method \texttt{zero\_grad()} can be used to set the gradients of all optimized parameters to zero. 

Currently, only a stochastic gradient descent optimizer \texttt{SGD} is available,that can be used with or without momentum. The implementation of momentum is based on the one in PyTorch which differs from Sutskever et. al. \cite{sutskever2013importance}. The update of parameter $p$ is defined as follows 
\begin{equation} \label{eq:SGDmoment}
  \begin{aligned}
    v &= \rho v + g \\
    p &= p - \alpha v,
  \end{aligned}
\end{equation}
where $\alpha$, $\rho$ and $g$ denote learning rate, momentum and parameter gradient respectively.

\section{Data Description}
According to the task description the training and test data sets consist each of 1000 points sampled uniformly in $[0,1]^2$. Class 0 is defined by all the points outside the disk with radius $\nicefrac{1}{\sqrt{2\pi}}$ and center $(\nicefrac{1}{2}, \nicefrac{1}{2})$ while class 1 denotes all the points inside that disk (see Figure~\ref{fig:data_set}).

\begin{figure}[h]
  \centering
  \includegraphics[width = 0.4\textwidth]{fig/disk_data_set.eps}
  \caption{Visualization of data set.}
  \label{fig:data_set}
\end{figure}


\section{Network Description}
The network used for classification of the data set consists of two input units, three hidden layers of 25 units and two output units. We chose to use ReLU as activation functions for all layers.

\section{Performance Evaluation}

\section{Conclusion}

\bibliographystyle{IEEEtran}
\bibliography{literature}

% reference to Bishop for Newton's method

\end{document}
